# 基础
### 三个基础问题 分类 回归 聚类
# 模型评估和选择
### 训练误差（经验误差）or 泛化误差  + 过拟合or欠拟合
**过拟合不可避免**
- 机器学习解决的问题大多>=NP问题 但算法是P, NP≠P
- 使用测试集近似泛化误差
- 验证集上选择模型和调超参数
    - ？？？
### 指标
**错误率和精度**
**查准率precision 和查全率recall**
- 一般相互矛盾
- 平衡点 包住

**ROC和AUC**
- 考察排序质量
 代价敏感错误率
- FP FN代价不同

**测试错误率和泛化错误率**
**偏差、方差和泛化误差**
![偏差、方差和泛化误差](image-1.png)
- 泛化误差= 偏差+方差+噪声
- 偏差 学习算法期望输出和真实值的差方 学习算法的拟合能力
- 方差 使用相同规模不同数据集训练下的f(x)的方差 数据扰动的影响
- 噪声 数据集中的标注和真实值的 差方   泛化误差的下界

---
# 线性模型
![线性模型](image.png)
### 二分类->多分类 
-  ECOC编码
### 类别不平衡问题
- 会讨好多数类 所以要人为提高少数类惩罚————再缩放
- grand也有同样的问题???? 随机采一批 正样本对 负样本对

# 决策树
![决策树](image-2.png)
### 信息熵与信息增益
- 将数据集的样本按属性a分类后 使得各个子树中的类别更加纯净
### 剪枝
- 预剪枝
![alt text](image-3.png)
- 后剪枝
![alt text](image-4.png)

---

# 神经网络
![alt text](image-6.png)
![alt text](image-5.png)

- 增加隐藏层提高网络的表达能力，层数足够多可以逼近任意函数
- 但是设置神经元数量是未知问题，只能试错
### 过拟合
- 由于表示能力强，容易过拟合
- 方法：早停
    - 验证集判断，是否训练集损失下降 验证集上升
    - 正则化 使得网络尽可能简单不复杂，weight decay 是一种方法
### 多层前馈神经网络
- 神经元之间不存在同层连接，也不存在跨层连接.这样的神经网络结构
### 全局最小  局部最小
- 不同初始化方法
- 模拟退火
- 随机梯度下降

# 支持向量机
![alt text](image-7.png)
![alt text](image-8.png)
- 核函数映射到高维特征空间
    - 核函数怎么选？
        - 匹配数据的分布特性
    - 有可训练参数吗？
        - 传统 SVM 的核函数参数是固定的，但现代核学习（如多核学习、自适应核）支持训练核函数的参数
    - 目前还有应用吗
        - 小样本、高维稀疏、对可解释性要求高的场景
- 高维特征空间超平面分割训练样本
- 软间隔和正则化
- 替代损失函数

# 贝叶斯分类器
求P(c | x )
1. 直接建模P(c | x)来预测c,这样得到的是“判别式模型" (discriminative models);
    - 决策树、BP 神经网络、支持向量机
2. 先对联合概率分布P(Q,c)建模，然后再由此获得P(c|x)，“生成式模型"(generative models).
    - 贝叶斯分类器
    - ![alt text](image-9.png)
    - 样本足够多可直接求P(c)，对各个类别p(x)都相同，可以抹去，最后求出各个类别看所占比例
    - 但P(x|c)无法求，因为x高维，取值太多，数据集不可能足够大
        - 但是可以假定x满足某种分布，然后拟合x的分布
        - P(x|c)是x多个属性的联合分布
            - 假设x各个属性独立 朴素
            - 假设依赖限制条件 半朴素 
            - 贝叶斯网 ![alt text](image-10.png)需要任务是求出网络结构————评分搜索

# 集成学习
![alt text](image-11.png)
- 子模块强依赖关系 串行
    - boosting
- 并行
    - bagging
    - 随机森林
# 基础
### 三个基础问题 分类 回归 聚类
# 模型评估和选择
### 训练误差（经验误差）or 泛化误差  + 过拟合or欠拟合
**过拟合不可避免**
- 机器学习解决的问题大多>=NP问题 但算法是P, NP≠P
- 使用测试集近似泛化误差
- 验证集上选择模型和调超参数
    - ？？？
### 指标
**错误率和精度**
**查准率precision 和查全率recall**
- 一般相互矛盾
- 平衡点 包住

**ROC和AUC**
- 考察排序质量
 代价敏感错误率
- FP FN代价不同

**测试错误率和泛化错误率**
**偏差、方差和泛化误差**
![偏差、方差和泛化误差](image-1.png)
- 泛化误差= 偏差+方差+噪声
- 偏差 学习算法期望输出和真实值的差方 学习算法的拟合能力
- 方差 使用相同规模不同数据集训练下的f(x)的方差 数据扰动的影响
- 噪声 数据集中的标注和真实值的 差方   泛化误差的下界
**一致损失替代函数**
**K 折交叉验证** 
- 将数据集 D 随机、均匀、无放回地划分为 K 个大小相等（或近似相等）的子集
- ![alt text](image-13.png)
- 更真实反映模型的泛化能力
---
# 线性模型
![线性模型](image.png)
### 二分类->多分类 
-  ECOC编码
### 类别不平衡问题
- 会讨好多数类 所以要人为提高少数类惩罚————再缩放
- grand也有同样的问题???? 随机采一批 正样本对 负样本对

# 决策树
![决策树](image-2.png)
### 信息熵与信息增益
- 将数据集的样本按属性a分类后 使得各个子树中的类别更加纯净
### 剪枝
- 预剪枝
![alt text](image-3.png)
- 后剪枝
![alt text](image-4.png)

---

# 神经网络
![alt text](image-6.png)
![alt text](image-5.png)

- 增加隐藏层提高网络的表达能力，层数足够多可以逼近任意函数
- 但是设置神经元数量是未知问题，只能试错
### 过拟合
- 由于表示能力强，容易过拟合
- 方法：早停
    - 验证集判断，是否训练集损失下降 验证集上升
    - 正则化 使得网络尽可能简单不复杂，weight decay 是一种方法
### 多层前馈神经网络
- 神经元之间不存在同层连接，也不存在跨层连接.这样的神经网络结构
### 全局最小  局部最小
- 不同初始化方法
- 模拟退火
- 随机梯度下降

# 支持向量机
![alt text](image-7.png)
![alt text](image-8.png)
- 核函数映射到高维特征空间
    - 核函数怎么选？
        - 匹配数据的分布特性
    - 有可训练参数吗？
        - 传统 SVM 的核函数参数是固定的，但现代核学习（如多核学习、自适应核）支持训练核函数的参数
    - 目前还有应用吗
        - 小样本、高维稀疏、对可解释性要求高的场景
- 高维特征空间超平面分割训练样本
- 软间隔和正则化
- 替代损失函数

# 贝叶斯分类器
求P(c | x )
1. 直接建模P(c | x)来预测c,这样得到的是“判别式模型" (discriminative models);
    - 决策树、BP 神经网络、支持向量机
2. 先对联合概率分布P(Q,c)建模，然后再由此获得P(c|x)，“生成式模型"(generative models).
    - 贝叶斯分类器
    - ![alt text](image-9.png)
    - 样本足够多可直接求P(c)，对各个类别p(x)都相同，可以抹去，最后求出各个类别看所占比例
    - 但P(x|c)无法求，因为x高维，取值太多，数据集不可能足够大
        - 但是可以假定x满足某种分布，然后拟合x的分布
        - P(x|c)是x多个属性的联合分布
            - 假设x各个属性独立 朴素
            - 假设依赖限制条件 半朴素 
            - 贝叶斯网 ![alt text](image-10.png)需要任务是求出网络结构————评分搜索

# 集成学习
![alt text](image-11.png)
- 集成学习理论基础是规模效应吗？
    - 泛化性 训练集上相当，但是在测试集上都偏离真实值，尽可能多的子分类器合并输出可以逼近真实值（基于统计）
    - 子分类器可能训练到局部最小
    - 多个子分类器可以增大假设空间,特征维度从n，变成T*n
- 子模块强依赖关系 串行
    - boosting 以adaboost算法为例
        - 训练多个分类器 最后加权求和分类器的结果
        - 样本权重 
            - 训练分类器 基于样本权重计算损失；
            - 上一个分类器分类不准，则该样本权重增大，准确则减小，变化比例是exp（分类器权重）的
        - 分类器权重 
            - 当前分类器训练后 的损失越大 权重越低
        - ![alt text](image-12.png)
        - 通过降低偏差 提高精度
- 并行
    - 想提高泛化性 需要使得个体学习器差异尽可能大
    - 降低方差
    - bagging
        - 为每个分类器随机采 m个样本，进行独立的训练
        - 所有分类器投票或平均
    - 随机森林
        - 构建多个独立的决策树，通过投票或平均的方式整合所有树的结果
        - 决策树节点分裂时 先随机选择k个属性，然后从k个属性中选一个最优属性分裂

- Stacking 算法 
    - 原始训练集 → [基学习器1, 基学习器2, ..., 基学习器k] → 基学习器预测结果 → 元学习器 → 最终预测
    - 基学习器可以是各种类型，训练数据集可以相同也可以不同
    - 元学习器 学习基学习器预测结果和真实标签的映射关系


# 聚类
- 距离度量
    - 闵可夫斯基距离 （有序属性）
        - ![alt text](image-14.png)
    - 无序属性 V D M (Value Difference Metric)
        - ![alt text](image-15.png)
- 原型聚类
    - 假设聚类结构能通过一组原型刻画
    - k均值
        - ![alt text](image-16.png)
        - ![alt text](image-18.png)
    - 学习向量量化
        - ![alt text](image-17.png)
        - 对任意样本叫它将被划入与其距离最近的原型向量所代表的簇中
    - 高斯混合聚类
- 非原型聚类
    - 密度聚类
    - 层次聚类
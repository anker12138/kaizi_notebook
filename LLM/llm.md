# Transformer核心架构（Encoder-Decoder/Decoder-only）知识点笔记

# 一、核心架构分类与核心定位

## 1. Encoder-Decoder架构

- 定位：「理解+生成」分工协作，适用于序列到序列（seq2seq）任务（翻译、摘要、SQL生成等）

- 核心组件：Encoder（负责全局理解输入，无因果掩码）+ Decoder（负责自回归生成，需因果掩码）+ 交叉注意力（Decoder关联Encoder输出，实现源-目标语义对齐）

## 2. Decoder-only架构

- 定位：纯生成架构，主流大模型（ChatGPT、LLaMA、通义千问）采用，适用于对话、通用生成等任务

- 核心特点：无Encoder，输入为「所有上下文拼接的单一序列」，依赖因果掩码和自回归生成逻辑

# 二、因果掩码核心知识

## 1. 定义与本质

上三角矩阵（对角线以上设为-∞），作用于Decoder的自注意力层，强制「每个位置只能关注前面的历史token，无法看到未来token」，保证时序因果一致性。

## 2. 核心作用（关键澄清）

- 不改变输入序列长度（物理长度），仅约束注意力可见范围（逻辑约束）；

- 不仅是「屏蔽多余内容」，更核心是「校准输入序列内部的时序依赖」（如输入[1,2,3]时，1不能看2/3，2不能看3）；

- 训练和推理阶段均必须启用，是自回归生成的核心保障。

## 3. 工作流程（以输入[1,2,3]为例）

- 生成3×3上三角掩码矩阵，对未来位置的注意力分数设为-∞；

- Softmax后，未来位置的注意力权重趋近于0，仅历史位置有效；

- 确保每个token的上下文表征基于「历史累积信息」，而非打乱时序的全量信息。

# 三、Encoder-Decoder：输入输出流动与训练/推理逻辑

## 1. 训练阶段

### （1）输入设计

- Encoder输入：源序列（如翻译任务的英文句子），带Padding掩码，无因果掩码；

- Decoder输入：目标序列前缀（真实标注的目标序列前n-1个token + \<s\>开始符），带双重掩码（Padding掩码+因果掩码）；

- 标签：目标序列后n-1个token + </s>结束符（与Decoder输入长度一致，实现一一对应预测）。

### （2）核心策略：Teacher Forcing

Decoder输入使用真实目标序列前缀，引导模型学习「源序列→目标序列」的映射，加速收敛；配合因果掩码避免偷看未来token。

## 2. 推理阶段（以英文→中文翻译为例）

### （1）输入输出流动

1. Encoder输入：待翻译英文序列，编码生成全局语义表征；

2. Decoder初始输入：仅\<s\>（无需手动输入中文前缀）；

3. 逐token生成：每次生成的中文token追加为新前缀，通过交叉注意力对齐Encoder输出，直到生成</s>或达到最大长度；

4. 最终输出：完整目标序列（中文翻译结果）。

### （2）关键补充

可手动输入中文前缀引导生成风格（如正式/口语化），但非必需，本质是优化生成效果的提示技巧。

# 四、Decoder-only：输入输出流动与训练/推理逻辑

## 1. 核心逻辑

将所有上下文（指令、历史对话、当前查询）拼接为单一序列，逐token自回归生成，输入输出共用一个序列，无需Encoder。

## 2. 推理阶段

### （1）输入输出流动（以对话生成为例）

1. 输入：拼接序列（\<s\> + 角色标记 + 历史对话 + 当前查询），带因果掩码；

2. 生成：每次预测当前序列的下一个token，生成后追加到输入序列，下一轮参考「原始输入+已生成token」；

3. 停止条件：生成</s>或达到上下文窗口最大长度。

### （2）关键疑问解答

- 是否参考自己生成的token？是！已生成token会追加为输入一部分，确保回答连贯性；

- 输入输出长度限制？有！受上下文窗口约束（如GPT-4o 128K、LLaMA 3 128K），总token数（输入+输出）不能超过窗口大小；

- 预测时参考真实输入还是生成token？初始预测（如输入[1,2,3]生成4）参考真实输入；后续生成（如生成5）参考「真实输入+已生成token」。

# 五、训练与推理的核心差异（通用）

|对比维度|训练阶段|推理阶段|
|---|---|---|
|Decoder输入来源|真实目标序列前缀（Teacher Forcing）|模型已生成的token（自回归拼接）|
|核心策略|用真实token引导，加速收敛|自回归生成，需缓解暴露偏差（调度采样、束搜索等）|
|解码方式|批量并行计算（一次计算全序列损失）|逐token串行生成（依赖KV缓存优化效率）|
|终止条件|无（固定长度序列损失计算）|生成</s>或达到最大序列长度|
# 六、关键优化技术

## 1. KV缓存

推理时缓存已生成token的K/V向量，后续生成仅计算新token的Q向量与缓存K/V的注意力，避免重复计算，提升效率。

## 2. 暴露偏差缓解

- 调度采样：训练后期用模型生成token替代部分真实token，贴近推理场景；

- 束搜索/Top-K/Top-P采样：提升推理生成质量与多样性。

# 七、高频疑问总结

1. Decoder能否一次性生成长度为n+10的输出？不能！需逐token迭代生成，每次前向传播仅预测1个token；

2. 输入[1,2,3]生成4时，Decoder会预测1、2、3吗？会计算但不输出，仅用于生成上下文表征，且KV缓存复用无额外计算量；

3. 因果掩码是否影响输入序列长度？不影响！仅约束注意力可见范围，输入/输出序列长度始终一致。

# 八、其他
1. 计费口径：输入 token（prompt / 上下文）+ 输出 token（模型回复），分别定价，输出更贵。
   1. 1M token≈75 万字中文
   2. 输入 每百万token 0.25~21$
   3. 输出 2~168$